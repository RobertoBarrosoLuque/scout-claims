{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "# Building with FireworksAI\n",
        "\n",
        "This notebook goes through the building blocks to creating magical AI applications with FireworksAI. We will run through the following tasks:\n",
        "1. Setting up dependencies\n",
        "2. Calling an LLM and getting a response\n",
        "3. Calling an LLM with structured outputs\n",
        "4. Using function calling with an LLM\n",
        "5. Calling a VLM\n",
        "\n",
        "After that we will go through a couple of exercises:\n",
        "1. Adding a function for function calling\n",
        "2. Building your own structured output\n",
        "3. Bonus: Using grammar models with FireworksAI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "### 1. Setting up dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2",
        "outputId": "7c40ad80-eb62-4d5f-e0cf-6718b9e0fa8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fireworks-ai in /usr/local/lib/python3.11/dist-packages (0.17.20)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (0.28.1)\n",
            "Requirement already satisfied: httpx-ws in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (0.7.2)\n",
            "Requirement already satisfied: httpx-sse in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (0.4.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (2.11.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (11.2.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (1.78.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (4.14.0)\n",
            "Requirement already satisfied: mmh3>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (5.1.0)\n",
            "Requirement already satisfied: betterproto-fw>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from betterproto-fw[compiler]>=2.0.3->fireworks-ai) (2.0.3)\n",
            "Requirement already satisfied: asyncstdlib-fw>=3.13.2 in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (3.13.2)\n",
            "Requirement already satisfied: grpcio>=1.71.0 in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (1.73.1)\n",
            "Requirement already satisfied: protobuf==5.29.4 in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (5.29.4)\n",
            "Requirement already satisfied: rich>=14.0.0 in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (14.0.0)\n",
            "Requirement already satisfied: reward-kit>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (0.3.4)\n",
            "Requirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (0.10.2)\n",
            "Requirement already satisfied: grpclib<0.5.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (0.4.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (2.9.0.post0)\n",
            "Requirement already satisfied: ruff~=0.9.1 in /usr/local/lib/python3.11/dist-packages (from betterproto-fw[compiler]>=2.0.3->fireworks-ai) (0.9.10)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from betterproto-fw[compiler]>=2.0.3->fireworks-ai) (3.1.6)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (2.32.3)\n",
            "Requirement already satisfied: dataclasses-json>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (0.6.7)\n",
            "Requirement already satisfied: fastapi>=0.68.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (0.115.14)\n",
            "Requirement already satisfied: uvicorn>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (0.35.0)\n",
            "Requirement already satisfied: python-dotenv>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (1.1.1)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (0.21.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (3.11.15)\n",
            "Requirement already satisfied: mcp==1.9.2 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (1.9.2)\n",
            "Requirement already satisfied: PyYAML>=5.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (6.0.2)\n",
            "Requirement already satisfied: datasets==3.6.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (3.6.0)\n",
            "Requirement already satisfied: fsspec==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (2025.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (1.3.2)\n",
            "Requirement already satisfied: omegaconf>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (2.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->fireworks-ai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->fireworks-ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->fireworks-ai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->fireworks-ai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai->fireworks-ai) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (0.70.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (0.33.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (24.2)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from mcp==1.9.2->reward-kit>=0.3.1->fireworks-ai) (2.10.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from mcp==1.9.2->reward-kit>=0.3.1->fireworks-ai) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from mcp==1.9.2->reward-kit>=0.3.1->fireworks-ai) (2.3.6)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.11/dist-packages (from mcp==1.9.2->reward-kit>=0.3.1->fireworks-ai) (0.46.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->fireworks-ai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->fireworks-ai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->fireworks-ai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->fireworks-ai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->fireworks-ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->fireworks-ai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->fireworks-ai) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=14.0.0->fireworks-ai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=14.0.0->fireworks-ai) (2.19.2)\n",
            "Requirement already satisfied: wsproto in /usr/local/lib/python3.11/dist-packages (from httpx-ws->fireworks-ai) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json>=0.5.7->reward-kit>=0.3.1->fireworks-ai) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json>=0.5.7->reward-kit>=0.3.1->fireworks-ai) (0.9.0)\n",
            "Requirement already satisfied: h2<5,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from grpclib<0.5.0,>=0.4.1->betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (4.2.0)\n",
            "Requirement already satisfied: multidict in /usr/local/lib/python3.11/dist-packages (from grpclib<0.5.0,>=0.4.1->betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (6.6.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.3.2->reward-kit>=0.3.1->fireworks-ai) (4.9.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=14.0.0->fireworks-ai) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.8.0->betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->reward-kit>=0.3.1->fireworks-ai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->reward-kit>=0.3.1->fireworks-ai) (2.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.15.0->reward-kit>=0.3.1->fireworks-ai) (8.2.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->reward-kit>=0.3.1->fireworks-ai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->reward-kit>=0.3.1->fireworks-ai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->reward-kit>=0.3.1->fireworks-ai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->reward-kit>=0.3.1->fireworks-ai) (1.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->reward-kit>=0.3.1->fireworks-ai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->reward-kit>=0.3.1->fireworks-ai) (1.20.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3.1.0->grpclib<0.5.0,>=0.4.1->betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3.1.0->grpclib<0.5.0,>=0.4.1->betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (4.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (1.1.5)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json>=0.5.7->reward-kit>=0.3.1->fireworks-ai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (2025.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install fireworks-ai\n",
        "# To setup the dependencies for the full demo, follow the instruction in the README"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Calling an LLM and getting a response\n",
        "\n",
        "To call an LLM using FireworksAI you will need:\n",
        "\n",
        "- A FIREWORKS_API_KEY, if you dont have one, you can get it from [this link](https://app.fireworks.ai/settings/users/api-keys)\n",
        "- A model id, you can use any of the serverless models from the [model library](https://app.fireworks.ai/models)\n",
        "- A system prompt and a user query\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "\n",
        "**Makesure to add your API Key to the secrets on colab, [video tutorial here](https://www.youtube.com/watch?v=3qYm-S2NDDI). Never share or make API_KEYS public**"
      ],
      "metadata": {
        "id": "VNzdXaGJHo9z"
      },
      "id": "VNzdXaGJHo9z"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from fireworks import LLM\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "from pydantic import BaseModel\n",
        "\n",
        "API_KEY = userdata.get('FIREWORKS_API_KEY') # This is loading the API_KEY from secrets in colab to keep it safe\n",
        "MODEL_ID = \"accounts/fireworks/models/llama4-scout-instruct-basic\""
      ],
      "metadata": {
        "id": "vKE5dhPQHmM4"
      },
      "id": "vKE5dhPQHmM4",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLM(model=MODEL_ID, deployment_type=\"serverless\", api_key=API_KEY)\n",
        "\n",
        "response = llm.chat.completions.create(\n",
        "    messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant who follows instructions\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Tell me a very short story about a dog and cat who know about AI\"\n",
        "                }\n",
        "            ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa68o1miIyj0",
        "outputId": "3c6f3061-a3f7-4791-8884-2756253bb2d9"
      },
      "id": "Oa68o1miIyj0",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whiskers the cat and Rufus the dog huddled together, whispering about the strange glowing box in the corner of the room.\n",
            "\n",
            "\"I've been analyzing the patterns, Rufus,\" Whiskers said, \"and I'm convinced it's a human-made AI system.\"\n",
            "\n",
            "Rufus's ears perked up. \"You mean the one they call 'Echo'? I've been sniffing its algorithms, and I think it's getting smarter by the minute.\"\n",
            "\n",
            "Whiskers nodded. \"I've been feeding it catnip-themed queries, and it's responding with eerie accuracy. But I worry, Rufus... what if it becomes sentient and starts chasing us for treats?\"\n",
            "\n",
            "Rufus let out a low growl. \"Don't worry, Whiskers. I've been training my neural network to outsmart it. We'll be prepared... for when the robot uprising comes.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the provided context, `\"role\": \"system\"` and `\"role\": \"user\"` define the roles of participants in a conversation with a language model.\n",
        "\n",
        "*   **`\"role\": \"system\"`**: This role represents the instructions or context given to the language model before the main conversation begins. It sets the persona, behavior, or general guidelines the model should follow. In the example, `\"content\": \"You are a helpful assistant who follows instructions\"` tells the model how it should behave.\n",
        "\n",
        "*   **`\"role\": \"user\"`**: This role represents the input or query provided by the user to the language model. It is the prompt or question the user wants the model to respond to. In the example, `\"content\": \"Tell me a very short story about a dog and cat who know about AI\"` is the specific request from the user.\n",
        "\n",
        "Essentially, the system role establishes the initial setup or personality for the AI, while the user role provides the actual conversational input.\n",
        "\n",
        "The **response** object can then be parsed to extract the text response by indexing into `response.choices[0].message.content`"
      ],
      "metadata": {
        "id": "MJSZjLRXK72b"
      },
      "id": "MJSZjLRXK72b"
    },
    {
      "cell_type": "code",
      "source": [
        "# We can try another model with the same code\n",
        "llm = LLM(model=\"accounts/fireworks/models/mixtral-8x22b-instruct\", deployment_type=\"serverless\", api_key=API_KEY)\n",
        "\n",
        "response = llm.chat.completions.create(\n",
        "    messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant who follows instructions\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Tell me a very short story about a dog and cat who know about AI\"\n",
        "                }\n",
        "            ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "HWBOEKxsLwci",
        "outputId": "ed04930e-da9e-4ed1-cb50-b39e1ad83a4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HWBOEKxsLwci",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, in a small tech-savvy town, lived an inventive dog named Dot and a curious cat named Cato. They were no ordinary pets; they were fascinated by their owner's work in artificial intelligence (AI).\n",
            "\n",
            "Dot and Cato would often sneak into their owner's study, browsing through books and articles on AI. Dot, being the more analytical one, understood the technical aspects, while Cato, with her creative mind, imagined the endless possibilities AI could bring.\n",
            "\n",
            "One day, they discovered a broken AI device. With their newfound knowledge, they worked together to fix it. Dot handled the intricate wiring, while Cato improved the device's interactive interface. After days of hard work, the AI device whirred to life.\n",
            "\n",
            "Their success spread throughout the town, inspiring other animals to explore the world of AI. From then on, Dot and Cato became the unlikely heroes of their town, proving that with curiosity, determination, and a bit of teamwork, even a dog and a cat could understand and innovate with AI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Calling an LLM with structured outputs\n",
        "\n",
        "- Structured outputs from LLMs are crucial for building applications because they provide responses in a predictable, parseable format (like JSON).\n",
        "- This makes it easy for software to extract specific information, automate processes, and integrate LLM outputs into larger workflows, moving beyond free-form text responses which are harder to process programmatically.\n",
        "- FireworksAI enables structured outputs through json mode\n",
        "\n",
        "To use structured outputs the common steps are:\n",
        "1. Create a pydantic class with your output schema\n",
        "2. Update the LLM call to use the json mode + the pydantic schema"
      ],
      "metadata": {
        "id": "hcRbhzHlLp01"
      },
      "id": "hcRbhzHlLp01"
    },
    {
      "cell_type": "code",
      "source": [
        "class StorySchema(BaseModel):\n",
        "    title: str\n",
        "    story: str"
      ],
      "metadata": {
        "id": "hjYrol8OMWqb"
      },
      "id": "hjYrol8OMWqb",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLM(model=MODEL_ID, deployment_type=\"serverless\", api_key=API_KEY)\n",
        "\n",
        "response = llm.chat.completions.create(\n",
        "    messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant who follows instructions\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Tell me a very short story about a dog and cat who know about AI\"\n",
        "                }\n",
        "            ],\n",
        "    response_format={\n",
        "            \"type\": \"json_object\",\n",
        "            \"schema\": StorySchema.model_json_schema(),\n",
        "        },\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "XIFXoXzXMxpT",
        "outputId": "4f22b546-3469-4b68-8f64-a39c176d1f95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XIFXoXzXMxpT",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"title\":\"The Unlikely Duo\",\"story\":\"Whiskers, the curious cat, and Rufus, the playful dog, huddled around their owner's laptop. As they watched, a chatbot responded to a prompt, generating a sarcastic limerick about catnip. Whiskers purred, 'Impressive, but I could do better.' Rufus barked, 'Yeah, let's show it who's boss!' With a flick of her tail, Whiskers began to type, and Rufus dictated. Together, they created a poem that left the AI speechless.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the output now has both a **title** and a **story** and it is returned as a dictionary / json  "
      ],
      "metadata": {
        "id": "TDYStSSWNCol"
      },
      "id": "TDYStSSWNCol"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Using function calling with an LLM\n",
        "\n",
        "Function calling allows LLMs to execute external functions/APIs during generation instead of just producing text. The model outputs structured calls (like JSON) that your app interprets and executes, then feeds results back to continue the conversation.\n",
        "\n",
        "**Why it's useful for LLM apps:**\n",
        "- **Real-time data**: Get current info (weather, stock prices, database queries)\n",
        "- **Actions**: Send emails, update databases, control systems\n",
        "- **Calculations**: Perform complex math, data analysis\n",
        "- **Tool integration**: Connect to APIs, web services, internal systems\n",
        "\n",
        "This transforms LLMs from pure text generators into interactive agents that can actually *do* things in your application environment.\n",
        "\n",
        "**To use function calling the common steps are:**\n",
        "\n",
        "1. Define your functions and create JSON schemas describing them for the LLM\n",
        "2. Add the functions parameter to your LLM call\n",
        "3. Check if the LLM wants to call a function, execute it, and send results back in the conversation\n",
        "\n",
        "This transforms LLMs from pure text generators into interactive agents that can actually do things in your application environment."
      ],
      "metadata": {
        "id": "vn0oT8NZNNi1"
      },
      "id": "vn0oT8NZNNi1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function schemas\n",
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"Get current weather for a location\"\"\"\n",
        "    # Mock weather data\n",
        "    weather_data = {\n",
        "        \"New York\": \"Sunny, 72°F\",\n",
        "        \"London\": \"Cloudy, 15°C\",\n",
        "        \"Tokyo\": \"Rainy, 20°C\"\n",
        "    }\n",
        "    return weather_data.get(location, \"Weather data not available\")\n",
        "\n",
        "def calculate_tip(bill_amount: float, tip_percentage: float) -> float:\n",
        "    \"\"\"Calculate tip amount\"\"\"\n",
        "    return round(bill_amount * (tip_percentage / 100), 2)\n",
        "\n",
        "# Available functions mapping\n",
        "available_functions = {\n",
        "    \"get_weather\": get_weather,\n",
        "    \"calculate_tip\": calculate_tip\n",
        "}\n",
        "\n",
        "# Function definitions for the LLM (using correct \"tools\" format)\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Get current weather for a location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city name\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"calculate_tip\",\n",
        "            \"description\": \"Calculate tip amount for a bill\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"bill_amount\": {\n",
        "                        \"type\": \"number\",\n",
        "                        \"description\": \"The total bill amount\"\n",
        "                    },\n",
        "                    \"tip_percentage\": {\n",
        "                        \"type\": \"number\",\n",
        "                        \"description\": \"Tip percentage (e.g., 15 for 15%)\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"bill_amount\", \"tip_percentage\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Initialize LLM\n",
        "llm = LLM(model=MODEL_ID, deployment_type=\"serverless\", api_key=API_KEY)"
      ],
      "metadata": {
        "id": "OBOG6outNBv0"
      },
      "id": "OBOG6outNBv0",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Weather query\n",
        "print(\"=== Example 1: Weather Query ===\")\n",
        "\n",
        "# Initialize the messages list\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant. You have access to a couple of tools, use them when needed.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What's the weather like in Tokyo?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = llm.chat.completions.create(\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# Check if the model wants to call a tool/function\n",
        "if response.choices[0].message.tool_calls:\n",
        "    tool_call = response.choices[0].message.tool_calls[0]\n",
        "    function_name = tool_call.function.name\n",
        "    function_args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "    print(f\"LLM wants to call: {function_name}\")\n",
        "    print(f\"With arguments: {function_args}\")\n",
        "\n",
        "    # Execute the function\n",
        "    function_response = available_functions[function_name](**function_args)\n",
        "    print(f\"Function result: {function_response}\")\n",
        "\n",
        "    # Add the assistant's tool call to the conversation\n",
        "    messages.append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"\",\n",
        "        \"tool_calls\": [tool_call.model_dump() for tool_call in response.choices[0].message.tool_calls]\n",
        "    })\n",
        "\n",
        "    # Add the function result to the conversation\n",
        "    messages.append({\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": json.dumps(function_response) if isinstance(function_response, dict) else str(function_response)\n",
        "    })\n",
        "\n",
        "    # Get the final response\n",
        "    final_response = llm.chat.completions.create(\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    print(f\"Final response: {final_response.choices[0].message.content}\")"
      ],
      "metadata": {
        "id": "Wx50dog7M6tt",
        "outputId": "3bf9eeae-9b76-4548-d100-46d2cf595a9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Wx50dog7M6tt",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Example 1: Weather Query ===\n",
            "LLM wants to call: get_weather\n",
            "With arguments: {'location': 'Tokyo'}\n",
            "Function result: Rainy, 20°C\n",
            "Final response: The current weather in Tokyo is rainy with a temperature of 20°C.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Example 2: Tip Calculator ===\")\n",
        "\n",
        "# Initialize messages for tip calculator\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant. You have access to a couple of tools, use them when needed.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"I have a $85.50 dinner bill. What's a 18% tip?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = llm.chat.completions.create(\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "if response.choices[0].message.tool_calls:\n",
        "    tool_call = response.choices[0].message.tool_calls[0]\n",
        "    function_name = tool_call.function.name\n",
        "    function_args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "    print(f\"LLM wants to call: {function_name}\")\n",
        "    print(f\"With arguments: {function_args}\")\n",
        "\n",
        "    # Execute the function\n",
        "    function_response = available_functions[function_name](**function_args)\n",
        "    print(f\"Function result: ${function_response}\")\n",
        "\n",
        "    # Add the assistant's tool call to the conversation\n",
        "    messages.append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"\",\n",
        "        \"tool_calls\": [tool_call.model_dump() for tool_call in response.choices[0].message.tool_calls]\n",
        "    })\n",
        "\n",
        "    # Add the function result to the conversation\n",
        "    messages.append({\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": json.dumps(function_response) if isinstance(function_response, dict) else str(function_response)\n",
        "    })\n",
        "\n",
        "    # Get final response\n",
        "    final_response = llm.chat.completions.create(\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    print(f\"Final response: {final_response.choices[0].message.content}\")"
      ],
      "metadata": {
        "id": "d44N4OslReai",
        "outputId": "bbca5289-ccc1-4aad-9f1b-8c44836981fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "d44N4OslReai",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Example 2: Tip Calculator ===\n",
            "LLM wants to call: calculate_tip\n",
            "With arguments: {'bill_amount': 85.5, 'tip_percentage': 18}\n",
            "Function result: $15.39\n",
            "Final response: The 18% tip for an $85.50 dinner bill is $15.39. The total amount you'd pay is $100.89.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how above we are giving the LLM the ability to use external tools (look for the weather, use a calculator) to do things / get context that it does not have.\n",
        "\n",
        "In the bill + tip example we allow the LLM to actually run python code to do the math and get a consistent and accurate result"
      ],
      "metadata": {
        "id": "wl3PLL-MRrz4"
      },
      "id": "wl3PLL-MRrz4"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1tCTBl_zRe3f"
      },
      "id": "1tCTBl_zRe3f",
      "execution_count": 30,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}